{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import estimagic as em"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap Monte Carlo Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this juypter notebook, we perform a Monte Carlo exercise to illustrate the importance of using the cluster robust variant of the bootstrap when data within clusters is correlated. \n",
    "\n",
    "The main idea is to repeatedly draw clustered samples, get both uniform and clustered bootstrap estimates in these samples, and then compare how often the true null hypothesis is rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generating Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true data generating process is given by\n",
    "\n",
    "$$ logit(y_{i,g}) = \\beta_0 + \\beta_1 (x_{i,g}) + \\epsilon_{i,g}, $$\n",
    "\n",
    "where the independent variable $x_{i,g} = x_i + x_g$ and the noise term $\\epsilon_{i,g} = \\epsilon_i + \\epsilon_g$ each consist of an individual and a cluster term.\n",
    "\n",
    "In the simulations we perform below, we have $\\beta_0 = \\beta_1 =0$. $x_i$ and $x_g$ are drawn from a standard normal distribution, and $\\epsilon_i$ and $\\epsilon_g$ are drawn from a normal distribution with $\\mu_0$ and $\\sigma=0.5$. The value of $\\sigma$ is chosen to not blow up rejection rates in the independent case too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clustered_data(nclusters, nobs_per_cluster, true_beta=0):\n",
    "    \"\"\"Create a bivariate clustered dataset with specified number of\n",
    "    clusters and number of observations per cluster that has a population\n",
    "    value of true_beta for the logit coefficient on the independent variable.\n",
    "\n",
    "    Args:\n",
    "        nclusters (int): Number of clusters.\n",
    "        nobs_per_cluster (int): Number of observations per cluster.\n",
    "        true_beta (int): The true logit coefficient on x.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Clustered dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    x_cluster = np.random.normal(size=nclusters)\n",
    "    x_ind = np.random.normal(size=nobs_per_cluster * nclusters)\n",
    "    eps_cluster = np.random.normal(size=nclusters, scale=0.5)\n",
    "    eps_ind = np.random.normal(size=nobs_per_cluster * nclusters, scale=0.5)\n",
    "\n",
    "    y = []\n",
    "    x = []\n",
    "    cluster = []\n",
    "\n",
    "    for g in range(nclusters):\n",
    "\n",
    "        for i in range(nobs_per_cluster):\n",
    "\n",
    "            key = (i + 1) * (g + 1) - 1\n",
    "\n",
    "            arg = (\n",
    "                true_beta * (x_cluster[g] + x_ind[key]) + eps_ind[key] + eps_cluster[g]\n",
    "            )\n",
    "\n",
    "            y_prob = 1 / (1 + np.exp(-arg))\n",
    "            y.append(np.random.binomial(n=1, p=y_prob))\n",
    "            x.append(x_cluster[g] + x_ind[(i + 1) * (g + 1) - 1])\n",
    "            cluster.append(g)\n",
    "\n",
    "    y = np.array(y)\n",
    "    x = np.array(x)\n",
    "    cluster = np.array(cluster)\n",
    "\n",
    "    return pd.DataFrame({\"y\": y, \"x\": x, \"cluster\": cluster})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Simulation Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function computes bootstrap t-values. As suggested my Cameron and Miller (2015), critical values are the 0.975 quantiles from a t distribution with `n_clusters` -1 degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_t_values(data, sample_size=200, hyp_beta=0, cluster=False):\n",
    "    \"\"\"Get bootstrap t-values for testing the hypothesis that beta == hyp_beta.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Original dataset.\n",
    "        sample_size (int): Number of bootstrap samples to draw.\n",
    "        hyp_beta (float): Hypothesised value of beta.\n",
    "        cluster (bool): Whether or not to cluster on the cluster column.\n",
    "\n",
    "    Returns:\n",
    "        float: T-Value of hypothesis.\n",
    "    \"\"\"\n",
    "\n",
    "    def logit_wrap(df):\n",
    "\n",
    "        y = df[\"y\"]\n",
    "        x = df[\"x\"]\n",
    "\n",
    "        result = sm.Logit(y, sm.add_constant(x)).fit(disp=0).params\n",
    "\n",
    "        return pd.Series(result, index=[\"constant\", \"x\"])\n",
    "\n",
    "    if cluster is False:\n",
    "\n",
    "        result = em.bootstrap(data=data, outcome=logit_wrap, n_draws=sample_size)\n",
    "        estimates = pd.DataFrame(result.outcomes())[\"x\"]\n",
    "\n",
    "    else:\n",
    "\n",
    "        result = em.bootstrap(\n",
    "            data=data,\n",
    "            outcome=logit_wrap,\n",
    "            n_draws=sample_size,\n",
    "            cluster_by=\"cluster\",\n",
    "        )\n",
    "        estimates = pd.DataFrame(result.outcomes())[\"x\"]\n",
    "\n",
    "    return (estimates.mean() - hyp_beta) / estimates.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo(nsim, nclusters, nobs_per_cluster, true_beta=0, n_cores=-1):\n",
    "    \"\"\"Run a Monte Carlo simulation for rejection rates and a logit data generating process.\n",
    "\n",
    "    Rejection rates are based on a t distribution with nclusters-1 degrees of freedom.\n",
    "\n",
    "    Args:\n",
    "        nsim (int): Number of Monte Carlo draws.\n",
    "        nclusters (int): Number of clusters in each generated dataset.\n",
    "        nobs_per_cluster (int) Number of observations per cluster.\n",
    "        true_beta (int): Population value of logit coefficient on x.\n",
    "        n_cores (int): Number of jobs for Parallelization.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame of average rejection rates.\n",
    "    \"\"\"\n",
    "\n",
    "    reject_independent = np.zeros(nsim)\n",
    "\n",
    "    reject_cluster = np.zeros(nsim)\n",
    "\n",
    "    def loop(i):\n",
    "\n",
    "        df = create_clustered_data(nclusters, nobs_per_cluster, true_beta)\n",
    "\n",
    "        return [get_t_values(df), get_t_values(df, cluster=True)]\n",
    "\n",
    "    t_value_array = np.array(\n",
    "        Parallel(n_jobs=n_cores)(delayed(loop)(i) for i in range(nsim))\n",
    "    )\n",
    "\n",
    "    crit = scipy.stats.t.ppf(0.975, nclusters - 1)\n",
    "\n",
    "    result = pd.DataFrame(np.abs(t_value_array) > crit, columns=[\"uniform\", \"cluster\"])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we perform Monte Carlo simulations with the above functions. In each simulation, the sample size is 200, but the number of clusters varies across simulations. Be warned that the code below takes a long time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(505)\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for g, k in [[20, 50], [50, 20], [100, 10], [200, 5], [500, 2]]:\n",
    "\n",
    "    results_list.append(monte_carlo(nsim=500, nclusters=g, nobs_per_cluster=k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rejection_data = pd.DataFrame([x.mean() for x in results_list])\n",
    "mean_rejection_data[\"nclusters\"] = [20, 50, 100, 200, 500]\n",
    "mean_rejection_data.set_index(\"nclusters\", inplace=True)\n",
    "\n",
    "print(mean_rejection_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = mean_rejection_data\n",
    "x = y.index.values\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "plt.xlabel(\"Number of clusters\", fontsize=12)\n",
    "plt.ylabel(\"Rejection rate\", fontsize=12)\n",
    "plt.plot(x, y[\"uniform\"], label=\"Uniform Bootstrap\", color=\"blue\", marker=\"o\")\n",
    "plt.plot(x, y[\"cluster\"], label=\"Cluster Bootstrap\", color=\"red\", marker=\"o\")\n",
    "plt.legend()\n",
    "plt.suptitle(\"Comparison of Rejection Rates\", fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that when the number of clusters is low, it is particularly important to use the cluster robust bootstrap, since rejection with the regular bootstrap is excessive. For a large number of clusters, clustering naturally becomes less important. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# How to do multistart optimizations\n",
    "\n",
    "Sometimes you want to make sure that your optimization is robust to the initial\n",
    "parameter values, i.e. that it does not get stuck at a local optimum. This is where\n",
    "multistart comes in handy.\n",
    "\n",
    "\n",
    "## What does multistart (not) do\n",
    "\n",
    "In short, multistart iteratively runs local optimizations from different initial\n",
    "conditions. If enough local optimization convergence to the same point, it stops.\n",
    "Importantly, it cannot guarantee that the result is the global optimum, but it can\n",
    "increase your confidence in the result.\n",
    "\n",
    "## TL;DR\n",
    "\n",
    "To activate multistart at the default options, pass `multistart=True` to the `minimize`\n",
    "or `maximize` function, as well as finite bounds on the parameters (which are used to\n",
    "sample the initial points). The default options are discussed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import optimagic as om\n",
    "\n",
    "\n",
    "def fun(x):\n",
    "    return x @ x\n",
    "\n",
    "\n",
    "x0 = np.arange(7) - 4\n",
    "\n",
    "res = om.minimize(\n",
    "    fun=fun,\n",
    "    x0=x0,\n",
    "    algorithm=\"scipy_neldermead\",\n",
    "    bounds=om.Bounds(\n",
    "        lower=np.full_like(x0, -3),\n",
    "        upper=np.full_like(x0, 8),\n",
    "    ),\n",
    "    multistart=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## What does multistart mean in optimagic?\n",
    "\n",
    "Our multistart optimizations are inspired by the [TikTak algorithm](https://github.com/serdarozkan/TikTak) and consist of the following steps:\n",
    "\n",
    "1. Draw a large exploration sample of parameter vectors randomly or using a\n",
    "   low-discrepancy sequence.\n",
    "1. Evaluate the objective function in parallel on the exploration sample.\n",
    "1. Sort the parameter vectors from best to worst according to their objective function\n",
    "   values. \n",
    "1. Run local optimizations iteratively. That is, the first local optimization is started\n",
    "   from the best parameter vector in the sample. All subsequent ones are started from a\n",
    "   convex combination of the currently best known parameter vector and the next sample\n",
    "   point. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Visualizing multistart results\n",
    "\n",
    "To illustrate the multistart results, we will consider the optimization of a slightly\n",
    "more complex objective function, compared to the `fun` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpine(x):\n",
    "    return np.sum(np.abs(x * np.sin(x) + 0.1 * x))\n",
    "\n",
    "\n",
    "res = om.minimize(\n",
    "    alpine,\n",
    "    x0=x0,\n",
    "    algorithm=\"scipy_neldermead\",\n",
    "    bounds=om.Bounds(lower=np.full_like(x0, -3), upper=np.full_like(x0, 8)),\n",
    "    multistart=om.MultistartOptions(n_samples=200, convergence_max_discoveries=3),\n",
    ")\n",
    "\n",
    "om.criterion_plot(res, max_evaluations=1_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "In the above image we see the optimization history for all of the local optimizations\n",
    "that have been run by multistart. The turquoise line represents the history\n",
    "corresponding to the local optimization that found the overall best parameter.\n",
    "\n",
    "We see that running a single optimization would not have sufficed, as some local\n",
    "optimizations are stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Multistart does not always run many optimization\n",
    "\n",
    "Since the local optimizations are run iteratively by multistart, it is possible that\n",
    "only a handful of optimizations are actually run, if all of them converge to the same\n",
    "point. This convergence is determined by the `convergence_max_discoveries` parameter,\n",
    "which defaults to 2. This means that if 2 local optimizations report the same point,\n",
    "multistart will stop. Below we see that if we use the simpler objective function\n",
    "(`fun`), with `convergence_max_discoveries` set to 3, multistart runs 3 local\n",
    "optimizations, and then stops, as all of them converge to the same point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = om.minimize(\n",
    "    fun,\n",
    "    x0=x0,\n",
    "    algorithm=\"scipy_neldermead\",\n",
    "    bounds=om.Bounds(lower=np.full_like(x0, -3), upper=np.full_like(x0, 8)),\n",
    "    multistart=om.MultistartOptions(n_samples=200, convergence_max_discoveries=3),\n",
    ")\n",
    "\n",
    "om.criterion_plot(res, max_evaluations=1_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## How to configure multistart?\n",
    "\n",
    "Configuration of multistart can be done by passing an instance of\n",
    "`optimagic.MultistartOptions` to `maximize` or `minimize`. Let's look at an extreme\n",
    "example where we manually set everything to it's default value:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "1. How to run a specific number of optimizations?\n",
    "1. How to set convergence options\n",
    "   - These are not the convergence criteria of the local optimizations\n",
    "1. How to configure the explorative sampling?\n",
    "    - Know you have good start values -> sample close to those\n",
    "    - No knowledge -> uniform\n",
    "    - Custom sample\n",
    "1. Parallelization\n",
    "    - n_cores\n",
    "    - batch_size\n",
    "    - batch_evaluator\n",
    "1. Randomness\n",
    "    - seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Understanding multistart results\n",
    "\n",
    "- Mention that the result is the same as the local optim result object of the best\n",
    "local optimization. Additional info is only found in the multistart_info attribute.\n",
    "\n",
    "\n",
    "When activating multistart, the optimization result object has the additional attribute\n",
    "`multistart_info`. It is a dictionary with the following keys:\n",
    "    \n",
    "- `local_optima`: A list with the results from all local optimizations that were performed.\n",
    "- `start_parameters`: A list with the start parameters from those optimizations \n",
    "- `exploration_sample`: A list with parameter vectors at which the objective function was evaluated in an initial exploration phase. \n",
    "- `exploration_results`: The corresponding objective values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Start parameters\n",
    "\n",
    "The start parameters are the parameter vectors from which the local optimizations were\n",
    "started. Since the default number of `convergence_max_discoveries` is 2, and both\n",
    "local optimizations were successfull, the start parameters have 2 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.multistart_info[\"start_parameters\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Local Optima\n",
    "\n",
    "The local optima are the results from the local optimizations. Since in this example\n",
    "only two local optimizations were run, the local optima list has two elements, each of\n",
    "which is an optimization result object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res.multistart_info[\"local_optima\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Exploration sample\n",
    "\n",
    "The exploration sample is a list of parameter vectors at which the objective function\n",
    "was evaluated. Since the parameter dimension is 5, and the default number of samples is\n",
    "100 times the number of parameters, the exploration sample has 500 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.row_stack(res.multistart_info[\"exploration_sample\"]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Exploration results\n",
    "\n",
    "The exploration results are the objective function values at the exploration sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res.multistart_info[\"exploration_results\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

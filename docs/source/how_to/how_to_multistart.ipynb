{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# How to do multistart optimizations\n",
    "\n",
    "Sometimes you want to make sure that your optimization is robust to the initial\n",
    "parameter values, i.e. that it does not get stuck at a local optimum. This is where\n",
    "multistart comes in handy.\n",
    "\n",
    "\n",
    "## What does multistart (not) do\n",
    "\n",
    "In short, multistart iteratively runs local optimizations from different initial\n",
    "conditions. If enough local optimization report convergence, it stops. Importantly, it\n",
    "cannot guarantee that the result is the global optimum, but it can increase your\n",
    "confidence in the result.\n",
    "\n",
    "## TL;DR\n",
    "\n",
    "To activate multistart at the default options, pass `multistart=True` to the `minimize`\n",
    "or `maximize` function, as well as finite soft bounds on the parameters (which are used\n",
    "to sample the initial points). The default options are discussed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import optimagic as om\n",
    "\n",
    "\n",
    "def fun(x):\n",
    "    return x @ x\n",
    "\n",
    "\n",
    "bounds = om.Bounds(\n",
    "    soft_lower=np.full(5, -5),\n",
    "    soft_upper=np.full(5, 10),\n",
    ")\n",
    "\n",
    "\n",
    "res = om.minimize(\n",
    "    fun=fun,\n",
    "    x0=np.arange(5),\n",
    "    algorithm=\"scipy_lbfgsb\",\n",
    "    bounds=bounds,\n",
    "    multistart=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## What does multistart mean in optimagic?\n",
    "\n",
    "Our multistart optimizations are inspired by the [TikTak algorithm](https://github.com/serdarozkan/TikTak) and consist of the following steps:\n",
    "\n",
    "1. Draw a large exploration sample of parameter vectors randomly or using a\n",
    "   low-discrepancy sequence.\n",
    "1. Evaluate the objective function in parallel on the exploration sample.\n",
    "1. Sort the parameter vectors from best to worst according to their objective function\n",
    "   values. \n",
    "1. Run local optimizations iteratively. That is, the first local optimization is started\n",
    "   from the best parameter vector in the sample. All subsequent ones are started from a\n",
    "   convex combination of the currently best known parameter vector and the next sample\n",
    "   point. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Visualizing multistart results\n",
    "\n",
    "To visualize the individual local optimization, you can call one of optimagics\n",
    "visualization function on the optimization result object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "om.criterion_plot(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "In the above figure you can see optimization history of two local optimizations. This\n",
    "means that the first two local optimizations were successfull, and the multistart\n",
    "optimization stopped after that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## How to configure multistart?\n",
    "\n",
    "Configuration of multistart can be done by passing an instance of\n",
    "`optimagic.MultistartOptions` to `maximize` or `minimize`. Let's look at an extreme\n",
    "example where we manually set everything to it's default value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = om.MultistartOptions(\n",
    "    # n_samples: The number of points at which the objective function is evaluated\n",
    "    #     during the exploration phase. If None, n_samples is set to 100 times the\n",
    "    #     number of parameters.\n",
    "    n_samples=100 * len(res.x),\n",
    "    # share_optimizations: The fraction of the exploration sample that is used to\n",
    "    #     run the optimization (relative to n_samples).\n",
    "    share_optimizations=0.1,\n",
    "    # sampling_distribution: The distribution from which the exploration sample is\n",
    "    #     drawn. Allowed are \"uniform\" and \"triangular\".\n",
    "    sampling_distribution=\"uniform\",\n",
    "    # sampling_method: The method used to draw the exploration sample. Allowed are\n",
    "    #     \"sobol\", \"random\", \"halton\", and \"latin_hypercube\".\n",
    "    sampling_method=\"sobol\",\n",
    "    # sample: A sequence of PyTrees that are used as the initial parameters for the\n",
    "    #     optimization. If None, a sample is drawn from the sampling distribution.\n",
    "    sample=None,\n",
    "    # mixing_weight_method: The method used to determine the mixing weight, i,e, how\n",
    "    #     start parameters for local optimizations are calculated. Allowed are\n",
    "    #     \"tiktak\" and \"linear\", or a custom callable.\n",
    "    mixing_weight_method=\"tiktak\",\n",
    "    # mixing_weight_bounds: The lower and upper bounds for the mixing weight.\n",
    "    mixing_weight_bounds=(0.1, 0.995),\n",
    "    # convergence_max_discoveries: The maximum number of discoveries for convergence.\n",
    "    #     Determines after how many re-descoveries of the currently best local\n",
    "    #     optima the multistart algorithm stops.\n",
    "    convergence_max_discoveries=2,\n",
    "    # convergence_relative_params_tolerance: The relative tolerance in parameters\n",
    "    #     for convergence. Determines the maximum relative distance two parameter\n",
    "    #     vecctors can have to be considered equal.\n",
    "    convergence_relative_params_tolerance=0.01,\n",
    "    # n_cores: The number of cores to use for parallelization.\n",
    "    n_cores=1,\n",
    "    # batch_evaluator: The evaluator to use for batch evaluation. Allowed are \"joblib\"\n",
    "    #     and \"pathos\", or a custom callable.\n",
    "    batch_evaluator=\"joblib\",\n",
    "    # batch_size: The batch size for batch evaluation. Must be larger than n_cores\n",
    "    #     or None.\n",
    "    batch_size=None,\n",
    "    # seed: The seed for the random number generator.\n",
    "    seed=None,\n",
    "    # exploration_error_handling: The error handling for exploration errors. Allowed\n",
    "    #     are \"raise\" and \"continue\".\n",
    "    exploration_error_handling=\"continue\",\n",
    "    # optimization_error_handling: The error handling for optimization errors. Allowed\n",
    "    #     are \"raise\" and \"continue\".\n",
    "    optimization_error_handling=\"continue\",\n",
    ")\n",
    "\n",
    "res = om.minimize(\n",
    "    fun=fun,\n",
    "    x0=np.arange(5),\n",
    "    algorithm=\"scipy_lbfgsb\",\n",
    "    bounds=bounds,\n",
    "    multistart=options,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Understanding multistart results\n",
    "\n",
    "When activating multistart, the optimization result object has the additional attribute\n",
    "`multistart_info`. It is a dictionary with the following keys:\n",
    "    \n",
    "- `local_optima`: A list with the results from all local optimizations that were performed.\n",
    "- `start_parameters`: A list with the start parameters from those optimizations \n",
    "- `exploration_sample`: A list with parameter vectors at which the objective function was evaluated in an initial exploration phase. \n",
    "- `exploration_results`: The corresponding objective values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Start parameters\n",
    "\n",
    "The start parameters are the parameter vectors from which the local optimizations were\n",
    "started. Since the default number of `convergence_max_discoveries` is 2, and both\n",
    "local optimizations were successfull, the start parameters have 2 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.multistart_info[\"start_parameters\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Local Optima\n",
    "\n",
    "The local optima are the results from the local optimizations. Since in this example\n",
    "only two local optimizations were run, the local optima list has two elements, each of\n",
    "which is an optimization result object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res.multistart_info[\"local_optima\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Exploration sample\n",
    "\n",
    "The exploration sample is a list of parameter vectors at which the objective function\n",
    "was evaluated. Since the parameter dimension is 5, and the default number of samples is\n",
    "100 times the number of parameters, the exploration sample has 500 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.row_stack(res.multistart_info[\"exploration_sample\"]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Exploration results\n",
    "\n",
    "The exploration results are the objective function values at the exploration sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res.multistart_info[\"exploration_results\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

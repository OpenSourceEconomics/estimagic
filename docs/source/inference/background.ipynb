{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Background on `likelihood_inference`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preamble"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This document will serve as the mathematical background to Estimagic's `likelihood_inference` functionality. The main part concerning `likelihood_inference` is with respect to the section: *Variance Estimation*."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Maximum-likelihood estimation seeks to maximize the likelihood function, defined as a function of an unknown parameter vector, **$\\beta$** , conditional on the data provided. For simplicity, we represent such a function as: $$L(\\beta|y_j, x_j)$$ where $y_j$ is the observed outcome variable for observation $j$ from the sample data and $x_j$ is the $j$th row of covariates for the same observation. Specifically, the function is called the **joint probability density** since the probability density function for each observation is seen as independent of one another and the distribution from which the observed outcome variable is generated stays the same for each observation [2]; thus, we seek to maximize the product of the densities $$ L(\\beta|y_j, x_j) = \\prod_{j=1}^{n} f(y_j| x_j, \\beta) $$ In practice, independence between observations might not hold or observations are weighted. For these deviations, we maximize the **pseudolikelihood function**, which provides good estimates of the parameters by using an approximation of the true joint probability density. Furthermore, the log of the pseudolikelihood function is used because it is simpler to maximize. The logarithm is a monotonic transformation, thus the parameters that maximize the product of the densities will also maximize the sum of the log-likelihoods. $$\\ln L(\\beta|y_j, x_j) = \\sum^n_{j=1} \\ln f(y_j|x_j, \\beta) $$ \n",
    "\n",
    "The sum of the densities is equal to the product of the densities when observations are independent. Deviation implies the sum is an *approximation* of the true likelihood. The functional form of $f$ depends on the distribution of the disturbance or outcome variable from the estimation model. For the present documentation, we consider the logistical distribution (logit) and the normal distribution (probit). These distirbutions work best for bounded outcome variables, where the probability of observing an outcome lies between 0 and 1 and the sum of the probabilities is equal to 1. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def estimate_likelihood_obs(params, y, x, criterion, design_options):\n",
    "    \"\"\"Pseudo-log-likelihood contribution per individual.\n",
    "\n",
    "    Args:\n",
    "        params (pd.DataFrame): The index consists of the parmater names,\n",
    "            the \"value\" column are the parameter values.\n",
    "        y (np.array): 1d numpy array with the dependent variable\n",
    "        x (np.array): 2d numpy array with the independent variables\n",
    "        criterion (string): \"logit\" or \"probit\"\n",
    "        design_options (pd.DataFrame): dataframe containing psu, stratum,\n",
    "            population/design weight and/or a finite population corrector (fpc)\n",
    "\n",
    "    Returns:\n",
    "        loglike (np.array): 1d numpy array with likelihood contribution per individual\n",
    "\n",
    "    \"\"\"\n",
    "    q = 2 * y - 1\n",
    "    if criterion == \"logit\":\n",
    "        c = np.log(1 / (1 + np.exp(-(q * np.dot(x, params[\"value\"])))))\n",
    "        if \"weight\" in design_options.columns:\n",
    "            return c * design_options[\"weight\"].to_numpy()\n",
    "        else:\n",
    "            return c\n",
    "    elif criterion == \"probit\":\n",
    "        c = np.log(stats.norm._cdf(np.dot(q[:, None] * x, params[\"value\"])))\n",
    "        if \"weight\" in design_options.columns:\n",
    "            return c * design_options[\"weight\"].to_numpy()\n",
    "        else:\n",
    "            return c\n",
    "    else:\n",
    "        print(\"Criterion function is misspecified or not supported.\")\n",
    "\n",
    "\n",
    "def estimate_likelihood(params, y, x, criterion, design_options):\n",
    "    \"\"\"Pseudo-log-likelihood.\n",
    "\n",
    "    Args:\n",
    "        params (pd.DataFrame): The index consists of the parameter names,\n",
    "            the \"value\" column are the parameter values.\n",
    "        y (np.array): 1d numpy array with the dependent variable\n",
    "        x (np.array): 2d numpy array with the independent variables\n",
    "        criterion (string): \"logit\" or \"probit\"\n",
    "        design_options (pd.DataFrame): dataframe containing psu, stratum,\n",
    "            population/design weight and/or a finite population corrector (fpc)\n",
    "\n",
    "    Returns:\n",
    "        loglike (np.array): 1d numpy array with sum of likelihood contributions\n",
    "\n",
    "    \"\"\"\n",
    "    return estimate_likelihood_obs(params, y, x, criterion, design_options).sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parameter Maximization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To find the vector of parameters which maximize the log-pseudolikelihood function, we take first order conditions of the likelihood with respect to the parameter vector: $$G(\\hat{\\beta}) = \\sum^n_{j=1} \\frac{\\partial \\ln L(\\hat{\\beta}|y_j, x_j)}{\\partial \\hat{\\beta}} = 0$$ The partial derivative of the log-likelihood for all observations with respect to the vector of parameters makes the **score vector** and we express the sum over all scores as $G(\\hat{\\beta})$. The existence of a global maximum depends on the shape of the function we are estimating and the parameter space. When the parameter space is compact and the log-likelihood is continuous on that space (i.e. for all parameter values, the log-likelihood returns a value), then a maximum exists. When the parameter space is convex and the log-likelihood f is strictly concave on the set of parameters, then there is a unique set of parameters which maximize the likelihood. \n",
    "\n",
    "To generate the parameters which maximize the log-likelihood function, we use Estimagic's `maximize` functionality. We use the L-BFGS-B algorithm. Running the algorithm returns a set of parameters which maximize the log-pseudolikelihood. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def estimate_parameters(log_like, design_options, log_like_kwargs, dashboard=False):\n",
    "    \"\"\"Estimate parameters that maximize log-likelihood function.\n",
    "\n",
    "    Args:\n",
    "        log_like (function): log-likelihood function\n",
    "        design_options (pd.DataFrame): dataframe containing psu, stratum,\n",
    "            population/design weight and/or a finite population corrector (fpc)\n",
    "        log_like_kwargs (dict): contains model equation, data, and type of\n",
    "            binary-choice model\n",
    "            Example:\n",
    "                log_like_kwargs = {\n",
    "                   \"formulas\": formulas,\n",
    "                   \"data\": orig_data,\n",
    "                   \"model\": \"logit\"\n",
    "                   }\n",
    "        dashboard (bool): Switch on the dashboard\n",
    "\n",
    "    Returns:\n",
    "        res: parameter vector and other optimization results.\n",
    "\n",
    "    \"\"\"\n",
    "    params, y, x = mle_processing(log_like_kwargs[\"formulas\"], log_like_kwargs[\"data\"])\n",
    "    res = maximize(\n",
    "        criterion=log_like,\n",
    "        params=params,\n",
    "        algorithm=\"scipy_L-BFGS-B\",\n",
    "        criterion_kwargs=log_like_kwargs,\n",
    "        dashboard=dashboard,\n",
    "    )\n",
    "    return res"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Variance Estimation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After parameter estimation, we extract the standard errors. To extract the standard-errors, we first solve for the variance-covariance matrix of the parameter vector. There are three methods to estimate the variance: \n",
    "\n",
    "(1) Taking the first order Taylor-Series expansion of $G(\\beta)$ and solving for $\\hat{V}(\\hat{\\beta})$. The result is the following: \n",
    "\n",
    "$$\\hat{V}(\\hat{\\beta}) = \\left[I(\\beta)\\right]^{-1} = -H^{-1}\\left[\\hat{V}\\{G(\\hat{\\beta})\\}\\right]-H^{-1}$$ \n",
    "\n",
    "for $\\beta=\\hat{\\beta}$. The **hessian** is denoted as $H$ and it is a $(k + 1) \\times (k + 1)$ matrix of second-derivatives of the log-likelihood \n",
    "\n",
    "$$H = \\left(\\frac{\\partial^2 \\ln L(\\beta|y_j, x_j)}{\\partial \\hat{\\beta} \\partial \\hat{\\beta}'}\\right)$$ \n",
    "\n",
    "where $k$ is number of independent variables except the constant. This method is referred to as the *sandwich estimator*, since the variance of $G(\\beta)$ is the meat between the inverse hessians. The representative function is the `sandwich_step` in `robust_se`. \n",
    "\n",
    "(2) The `observed_information_matrix`, which is the inverse of the Hessian matrix. This returns the variance-covariance matrix of the paramters and square-rooting the diagonal of variances, returns the standard errors. \n",
    "\n",
    "(3) `outer_product_of_gradients`. Taking the inverse of the product of two jacobian matrices returns the variance-covariance matrix for the parameters and likewise, returns the standard errors from squarerooting the diagonal. This is the least most computationally demanding given second order conditions can be difficult to derive and the jacobian has already been calculated. The estimator is $$\\left[I(\\beta)\\right]^{-1} = \\left[\\sum^n_{j=1} g_j'g_j\\right]^{-1} $$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "def observed_information_matrix(hess):\n",
    "    \"\"\"Observed information matrix or BHHH estimator.\n",
    "\n",
    "    Args:\n",
    "        hess (np.array): \"hessian\" - a k + 1 x k + 1-dimensional array of\n",
    "            second derivatives of the pseudo-log-likelihood function w.r.t.\n",
    "            the parameters\n",
    "    Returns:\n",
    "        oim_se (np.array): a 1d array of k + 1 standard errors\n",
    "        oim_var (np.array): 2d variance-covariance matrix\n",
    "\n",
    "    \"\"\"\n",
    "    hess = -hess.copy()\n",
    "    oim_var = np.linalg.inv(hess)\n",
    "    oim_se = np.sqrt(np.diag(oim_var))\n",
    "    return oim_se, oim_var\n",
    "\n",
    "\n",
    "def outer_product_of_gradients(jac):\n",
    "    \"\"\"Outer product of gradients estimator.\n",
    "\n",
    "    Args:\n",
    "        jac (np.array): \"jacobian\" - an n x k + 1-dimensional array of first\n",
    "            derivatives of the pseudo-log-likelihood function w.r.t. the parameters\n",
    "\n",
    "    Returns:\n",
    "        opg_se (np.array): a 1d array of k + 1 standard errors\n",
    "        opg_var (np.array): 2d variance-covariance matrix\n",
    "\n",
    "    \"\"\"\n",
    "    opg_var = np.linalg.inv(np.dot(jac.T, jac))\n",
    "    opg_se = np.sqrt(np.diag(opg_var))\n",
    "    return opg_se, opg_var\n",
    "\n",
    "\n",
    "def sandwich_step(hess, meat):\n",
    "    \"\"\"The sandwich estimator for variance estimation.\n",
    "\n",
    "    Args:\n",
    "        hess (np.array): \"hessian\" - a k + 1 x k + 1-dimensional array of\n",
    "            second derivatives of the pseudo-log-likelihood function w.r.t.\n",
    "            the parameters\n",
    "        meat (np.array): the variance of the total scores\n",
    "\n",
    "    Returns:\n",
    "        se (np.array): a 1d array of k + 1 standard errors\n",
    "        var (np.array): 2d variance-covariance matrix\n",
    "\n",
    "    \"\"\"\n",
    "    invhessian = np.linalg.inv(hess)\n",
    "    var = np.dot(np.dot(invhessian, meat), invhessian)\n",
    "    se = np.sqrt(np.diag(var))\n",
    "    return se, var"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The inner component, $\\hat{V}\\{G(\\hat{\\beta})\\}$, is the variance of a sum of partial derivatives. The variance of any sum is: $$\\frac{n}{n-1} \\sum^{n}_{j=1}(u_j-\\overline{u_j})^2$$ The mean of the score vector is zero, thus $\\overline{u_j} =0$. Thus, the robust variance estimator is: \n",
    "\n",
    "$$\\hat{V}(\\hat{\\beta}) = \\left[-H^{-1}\\left(\\frac{n}{n-1} \\sum^{n}_{j=1}(u^{'}_ju_j)\\right)-H^{-1}\\right]$$\n",
    "\n",
    "The use of this variance estimator is justified for independent observations. In the case where a group of observations are correlated, we have to adjust the estimator to take the sum over clusters compared to taking sums over individual observations. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def robust_se(jac, hess):\n",
    "    \"\"\"Robust standard errors.\n",
    "\n",
    "    Args:\n",
    "        jac (np.array): \"jacobian\" - an n x k + 1-dimensional array of first\n",
    "            derivatives of the pseudo-log-likelihood function w.r.t. the parameters\n",
    "        hess (np.array): \"hessian\" - a k + 1 x k + 1-dimensional array of second derivatives\n",
    "            of the pseudo-log-likelihood function w.r.t. the parameters\n",
    "\n",
    "    Returns:\n",
    "        se (np.array): a 1d array of k + 1 standard errors\n",
    "        var (np.array): 2d variance-covariance matrix\n",
    "\n",
    "    \"\"\"\n",
    "    sum_scores = np.dot((jac).T, jac)\n",
    "    meat = (len(jac) / (len(jac) - 1)) * sum_scores\n",
    "    se, var = sandwich_step(hess, meat)\n",
    "    return se, var"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The cluster-robust variance is: \n",
    "\n",
    "$$\\hat{V}(\\hat{\\beta}) = \\left[-H^{-1}\\left(\\frac{n_c}{n_c-1} \\sum^{n_c}_{i=1}\\left(\\sum_{j\\in C_i}u_{j}\\right)^{'}\\left(\\sum_{j\\in C_i}u_{j}\\right)\\right)-H^{-1}\\right]$$ \n",
    "\n",
    "where $n_c$ refers to the number of clusters and $\\sum_{j\\in C_i}u_j$ is the sum of the scores in cluster $j$. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def clustering(design_options, jac):\n",
    "    \"\"\"Variance estimation for each cluster.\n",
    "\n",
    "    The function takes the sum of the jacobian observations for each cluster.\n",
    "    The result is the meat of the sandwich estimator.\n",
    "\n",
    "    Args:\n",
    "        design_options (pd.DataFrame): dataframe containing psu, stratum,\n",
    "            population/design weight and/or a finite population corrector (fpc)\n",
    "        jac (np.array): \"jacobian\" - an n x k + 1-dimensional array of first\n",
    "            derivatives of the pseudo-log-likelihood function w.r.t. the parameters\n",
    "\n",
    "    Returns:\n",
    "        cluster_meat (np.array): 2d square array of length k + 1. Variance of\n",
    "            the likelihood equation (Pg.557, 14-10, Greene 7th edition)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    list_of_clusters = design_options[\"psu\"].unique()\n",
    "    meat = np.zeros([len(jac[0, :]), len(jac[0, :])])\n",
    "    for psu in list_of_clusters:\n",
    "        psu_scores = jac[design_options[\"psu\"] == psu]\n",
    "        psu_scores_sum = psu_scores.sum(axis=0)\n",
    "        meat += np.dot(psu_scores_sum[:, None], psu_scores_sum[:, None].T)\n",
    "    cluster_meat = len(list_of_clusters) / (len(list_of_clusters) - 1) * meat\n",
    "    return cluster_meat\n",
    "\n",
    "\n",
    "def cluster_robust_se(jac, hess, design_options):\n",
    "    \"\"\"Cluster robust standard errors.\n",
    "\n",
    "    A cluster is a group of observations that correlate amongst each other,\n",
    "    but not between groups. Each cluster is seen as independent. As the number\n",
    "    of clusters increase, the standard errors approach robust standard errors.\n",
    "\n",
    "    Args:\n",
    "        jac (np.array): \"jacobian\" - an n x k + 1-dimensional array of first\n",
    "            derivatives of the pseudo-log-likelihood function w.r.t. the parameters\n",
    "        hess (np.array): \"hessian\" - a k + 1 x k + 1-dimensional array of\n",
    "            second derivatives of the pseudo-log-likelihood function w.r.t.\n",
    "            the parameters\n",
    "    Returns:\n",
    "        cluster_robust_se (np.array): a 1d array of k + 1 standard errors\n",
    "        cluster_robust_var (np.array): 2d variance-covariance matrix\n",
    "\n",
    "    \"\"\"\n",
    "    cluster_meat = clustering(design_options, jac)\n",
    "    cluster_robust_se, cluster_robust_var = sandwich_step(hess, cluster_meat)\n",
    "    return cluster_robust_se, cluster_robust_var"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "`likelihood_inference` also allows for strata. Strata (plural) are constructed using information (e.g., age, income, gender). For illustration, suppose I have two age categories, \"old\" and \"young\" and two income categories, \"low-income\" and \"high-income\". The strata given this information would be the combinations: (old, low-income), (old, high-income), (young, low-income), and (young, high-income). Stratification is used as a variance reduction method. Observe this split of the data as defining sub-populations from the larger population. Strata may contain independent observations or clusters (psu's). The strata-robust variance is: $$\\hat{V}(\\hat{\\beta}) = \\left[-H^{-1}\\left(\\sum^L_{h=1}(1 - f_h)\\frac{n_c}{n_c-1} \\sum^{n_c}_{j=1}\\left(\\sum_{j\\in C_i}u_j - \\overline{u_j}\\right)^{'}\\left(\\sum_{j\\in C_i}u_j - \\overline{u_j}\\right)\\right)-H^{-1}\\right]$$ where $L$ refers to how many stratum, $h$, there are, $n_c$ refers to the number of clusters in $h$ and $\\sum_{j\\in C_i}u_j$ is the sum of the scores in cluster $j$ for strata $h$. $\\overline{u_j}$ does not equal zero here, since we take the mean of the jacobian vector for that strata; thus, a subset of the data. Otherwise, for the entire sample, the mean of the jacobian vector is zero, as in cluster robust. The calculation is roughly the same as `cluster_robust_se`, except the calculation is done for each strata. $(1 - f_h)$ is the finite population corrector, where its default value is $1$ ($f_h = 0$). "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The process is the following:\n",
    "\n",
    "1)  Each cluster or psu contains at least one observation, but typically more than one. We first take the sum of the jacobian observations in that cluster. If the cluster has 10 observations, we take the sum of those 10 jacobian observations, returning one row. We do this for all clusters in strata $h$ and stack the sums, to create $n_c \\times (k + 1)$ matrix. \n",
    "\n",
    "2) To calculate $\\overline{u_j}$, we take the mean of each column of $u_j$ and subtract it from that same column.\n",
    "\n",
    "3) Then, we transpose the matrix and multiply it by the untransposed version of it. This represents $\\sum^{n_c}_{j=1}\\left(\\sum_{j\\in C_i}u_j - \\overline{u_j}\\right)^{'}\\left(\\sum_{j\\in C_i}u_j - \\overline{u_j}\\right)$. \n",
    "\n",
    "4) We multiply this sum by $\\frac{n_c}{n_c-1}$, where $n_c$ is the number of clusters in that particular strata. We also multiply the sum by the finite population corrector if provided. \n",
    "\n",
    "5) Repeat this process for all strata and sum all the resulting matrices. The size of the matrix for each iteration in the sum is always $(k + 1)\\times (k + 1)$. This is of course expected because the result is the variance-covariance matrix. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def stratification(design_options, jac):\n",
    "    \"\"\"Variance estimatio for each strata stratum.\n",
    "\n",
    "    The function takes the sum of the jacobian observations for each cluster\n",
    "    within strata. The result is the meat of the sandwich estimator.\n",
    "\n",
    "    Args:\n",
    "        design_options (pd.DataFrame): dataframe containing psu, stratum,\n",
    "            population/design weight and/or a finite population corrector (fpc)\n",
    "        jac (np.array): \"jacobian\" - an n x k + 1-dimensional array of first\n",
    "            derivatives of the pseudo-log-likelihood function w.r.t. the parameters\n",
    "\n",
    "    Returns:\n",
    "        strata_meat (np.array): 2d square array of length k + 1. Variance of\n",
    "        the likelihood equation\n",
    "\n",
    "    \"\"\"\n",
    "    n_params = len(jac[0, :])\n",
    "    stratum_col = design_options[\"strata\"]\n",
    "    # Stratification does not require clusters\n",
    "    if \"psu\" not in design_options:\n",
    "        design_options[\"psu\"] = design_options.index\n",
    "    else:\n",
    "        pass\n",
    "    psu_col = design_options[\"psu\"]\n",
    "    strata_meat = np.zeros([n_params, n_params])\n",
    "    # Variance estimation per stratum\n",
    "    for stratum in stratum_col.unique():\n",
    "        psu_in_strata = psu_col[stratum_col == stratum].unique()\n",
    "        psu_jac = np.zeros([n_params])\n",
    "        if \"fpc\" in design_options:\n",
    "            fpc = design_options[\"fpc\"][stratum_col == stratum].unique()\n",
    "        else:\n",
    "            fpc = 1\n",
    "        # psu_jac stacks the sum of the observations for each cluster.\n",
    "        for psu in psu_in_strata:\n",
    "            psu_jac = np.vstack([psu_jac, np.sum(jac[psu_col == psu], axis=0)])\n",
    "        psu_jac_mean = np.sum(psu_jac, axis=0) / len(psu_in_strata)\n",
    "        if len(psu_in_strata) > 1:\n",
    "            mid_step = np.dot(\n",
    "                (psu_jac[1:] - psu_jac_mean).T, (psu_jac[1:] - psu_jac_mean)\n",
    "            )\n",
    "            strata_meat += (\n",
    "                fpc * (len(psu_in_strata) / (len(psu_in_strata) - 1)) * mid_step\n",
    "            )\n",
    "        # Apply \"grand-mean\" method for single unit stratum\n",
    "        elif len(psu_in_strata) == 1:\n",
    "            strata_meat += fpc * np.dot(psu_jac[1:].T, psu_jac[1:])\n",
    "\n",
    "    return strata_meat\n",
    "\n",
    "\n",
    "def strata_robust_se(jac, hess, design_options):\n",
    "    \"\"\"Cluster robust standard errors.\n",
    "\n",
    "    A stratum is a group of observations that share common information. Each\n",
    "    stratum can be constructed based on age, gender, education, region, etc.\n",
    "    The function runs the same formulation for cluster_robust_se for each\n",
    "    stratum and returns the sum. Each stratum contain primary sampling units\n",
    "    (psu) or clusters. If observations are independent, but wish to have to\n",
    "    strata, make the psu column take the values of the index.\n",
    "\n",
    "    Args:\n",
    "        jac (np.array): \"jacobian\" - an n x k + 1-dimensional array of first\n",
    "            derivatives of the pseudo-log-likelihood function w.r.t. the parameters\n",
    "        hess (np.array): \"hessian\" - a k + 1 x k + 1-dimensional array of\n",
    "            second derivatives of the pseudo-log-likelihood function w.r.t.\n",
    "            the parameters\n",
    "        design_options (pd.DataFrame): dataframe containing psu, stratum,\n",
    "            population/design weight and/or a finite population corrector (fpc)\n",
    "\n",
    "    Returns:\n",
    "        strata_robust_se (np.array): a 1d array of k + 1 standard errors\n",
    "        strata_robust_var (np.array): 2d variance-covariance matrix\n",
    "\n",
    "    \"\"\"\n",
    "    strata_meat = stratification(design_options, jac)\n",
    "    strata_robust_se, strata_robust_var = sandwich_step(hess, strata_meat)\n",
    "    return strata_robust_se, strata_robust_var"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### References "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[1] Stata 13 Base Reference Manual. Section: robust. College Station, TX: Stata Press. StataCorp. 2013.\n",
    "\n",
    "[2] Greene, William H. Econometric analysis. Pearson Education India, 2003.\n",
    "\n",
    "[3] Taboga, Marco. Lectures on Probability Theory and Mathematical Statistics - 3rd Edition. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

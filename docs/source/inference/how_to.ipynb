{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `likelihood_inference` Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using simplified examples, we demonstrate how to use our functionality. We also provide side-by-side comparison to Stata results for robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to use our functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functionality is for statistical inference of maximum-likelihood estimations. The function also accepts design options for survey data. For details, please consult our background page [here]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary objects "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use our functionality, we need to define a few objects first. \n",
    "\n",
    "1) First, define your **dataset** as a pandas dataframe object. \n",
    "\n",
    "2) Second, define your model. We define a `logit` and `probit` function for illustration. \n",
    "\n",
    "3) Third, construct a dictionary containing the keys equal to the model's arguments and the values equal to whatever make up  those keys. This will be more clear below. This will be your `log_like_kwargs`.\n",
    "\n",
    "4) Finally, create a **design options dictionary**, where the keys acceptable keys are \"psu\", \"strata\", \"weight\", and \"fpc\". The values are the column names from the dataset that correspond to that key. For example, if you wish to cluster by school, you would define the key-value pair as `{\"psu\": data[\"school\"]}`. Given the variety of survey data and what they provide for statistical inference, the design dictionary can be empty, contain only weight, only the primary sampling unit (also referred to as cluster) and/or other variations of the four options. The dictionary is then converted into a **dataframe**. This is **not optional**. If you wish to not specify any design options, toss the function an empty pandas dataframe.  \n",
    "\n",
    "\n",
    "Below, we present an example of the necessary objects and three illustrations of the function in action. \n",
    "To illustrate, we have programmed a logit and probit function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from patsy import dmatrices\n",
    "\n",
    "from estimagic.inference.src.functions.mle_unconstrained import mle_processing\n",
    "from estimagic.inference.src.functions.se_estimation import choose_case\n",
    "from estimagic.inference.src.functions.se_estimation import cluster_robust_se\n",
    "from estimagic.inference.src.functions.se_estimation import clustering\n",
    "from estimagic.inference.src.functions.se_estimation import inference_table\n",
    "from estimagic.inference.src.functions.se_estimation import \\\n",
    "    likelihood_inference\n",
    "from estimagic.inference.src.functions.se_estimation import np_hess\n",
    "from estimagic.inference.src.functions.se_estimation import np_jac\n",
    "from estimagic.inference.src.functions.se_estimation import \\\n",
    "    observed_information_matrix\n",
    "from estimagic.inference.src.functions.se_estimation import \\\n",
    "    outer_product_of_gradients\n",
    "from estimagic.inference.src.functions.se_estimation import robust_se\n",
    "from estimagic.inference.src.functions.se_estimation import strata_robust_se\n",
    "from estimagic.inference.src.functions.se_estimation import stratification\n",
    "from estimagic.inference.src.functions.se_estimation import variance_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(params, y, x, design_options):\n",
    "    \"\"\"Pseudo-log-likelihood contribution per individual.\n",
    "\n",
    "    Args:\n",
    "        params (pd.DataFrame): The index consists of the parmater names,\n",
    "            the \"value\" column are the parameter values.\n",
    "        y (np.array): 1d numpy array with the dependent variable\n",
    "        x (np.array): 2d numpy array with the independent variables\n",
    "        design_options (pd.DataFrame): dataframe containing psu, stratum,\n",
    "            population/design weight and/or a finite population corrector (fpc)\n",
    "\n",
    "    Returns:\n",
    "        c (np.array): 1d numpy array with likelihood contribution per individual\n",
    "\n",
    "    \"\"\"\n",
    "    q = 2 * y - 1\n",
    "    # likelihood contribution\n",
    "    c = np.log(1 / (1 + np.exp(-(q * np.dot(x, params[\"value\"])))))\n",
    "    if \"weight\" in design_options:\n",
    "        return c * design_options[\"weight\"].to_numpy()\n",
    "    else:\n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Create logit keyword arguments\n",
    "formula = \"eco_friendly ~ ppltrst + male + income\"\n",
    "y, x = dmatrices(formula, data, return_type=\"dataframe\")\n",
    "y = y[y.columns[0]]\n",
    "design_options = pd.DataFrame()\n",
    "\n",
    "logit_kwargs = {\"y\": y, \"x\": x, \"design_options\": design_options}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`logit` takes `params`, `y`, `x`, and `design_options`. Above, `logit_kwargs` takes three arguments. The `params` argument in `likelihood_inference` below is already the estimated parameters. Estimagic uses the `maximize` function to estimate parameters; refer to their documentation [here](https://estimagic.readthedocs.io/en/latest/optimization/interface.html). Also, although we specified `design_options` in our `log_like_kwargs`, this is only because our `logit` function can take a weight for the contribution of the likelihood. Otherwise, you would just toss it in `likelihood_inference` (otherwise here would be if you just specify a cluster, strata and/or finite population corrector and not a weight). Weights affect parameter estimation, hence why it goes in before. Otherwise, it is unneccessary.\n",
    "\n",
    "`likelihood_inference` then takes `logit` as `log_like_obs`, estimated params, `logit_kwargs` as `log_like_kwargs` and `design_options` as `design_options`. `cov_type` tells the function which variance estimator to use. We allow for three options: (1) `observed_information_matrix` or `\"hessian\"` (2) `outer_product_of_gradients` or `\"jacobian\"` and (3) White's standard errors or `\"sandwich\"`. The default is `\"jacobian\"`. Explanations and details on each of these estimators can be found in background or in the docstring below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_inference(\n",
    "    log_like_obs, params, log_like_kwargs, design_options, cov_type=\"jacobian\"\n",
    "):\n",
    "    \"\"\"Pseudolikelihood estimation and inference.\n",
    "\n",
    "    Args:\n",
    "        log_like_obs (func): The pseudo-log-likelihood function. It is the\n",
    "            log-likelihood contribution per individual.\n",
    "        params (pd.DataFrame): The index consists of the paramater names specified\n",
    "            by the user, the \"value\" column is the parameter values.\n",
    "        log_like_kwargs (dict): In addition to the params argument directly\n",
    "            taken by likelihood_inference function, additional keyword arguments for the\n",
    "            likelihood function may include dependent variable, independent variables\n",
    "            and design options.\n",
    "            Example of simple logit model arguments:\n",
    "                log_like_kwargs = {\n",
    "                    \"y\": y,\n",
    "                    \"x\": x,\n",
    "                    \"design_options\": design_options\n",
    "                }\n",
    "        design_options (pd.DataFrame): dataframe containing psu, stratum,\n",
    "            population/design weight and/or a finite population corrector (fpc)\n",
    "        cov_type (str): One of [\"opg\", \"oim\", \"sandwich\"]. opg and oim only\n",
    "            work when *design_options* is empty. opg is default.\n",
    "\n",
    "    Returns:\n",
    "        model_inference_table (pd.DataFrame):\n",
    "            - \"value\": params that maximize likelihood\n",
    "            - \"standard_error\": standard errors of the params\n",
    "            - \"ci_lower\": using the 95% critical value of a normal distribution * -1\n",
    "            - \"ci_upper\": using the 95% critical value of a normal distribution\n",
    "        params_cov (pd.DataFrame): Covariance matrix of estimated parameters.\n",
    "            Index and columns are the same as params.index.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "        >>> from estimagic.inference.sample_models import logit\n",
    "        >>> cc = choose_case\n",
    "        >>> params = pd.DataFrame(data=[0.5, 0.5], columns=[\"value\"])\n",
    "        >>> x = np.array([[1., 5.], [1., 6.]])\n",
    "        >>> y = np.array([[1., 1]])\n",
    "        >>> d_opt = pd.DataFrame()\n",
    "        >>> logit_kwargs = {\"y\": y, \"x\": x, \"design_options\": d_opt}\n",
    "        >>> se, var = cc(logit, params, logit_kwargs, d_opt, cov_type=\"jacobian\")\n",
    "        >>> se, var\n",
    "        (array([212.37277788,  40.10565957]), array([[45102.19678307, -8486.9195158 ],\n",
    "               [-8486.9195158 ,  1608.46392969]]))\n",
    "\n",
    "        >>> inf_table, cov = inference_table(params, se, var, cov_type=\"jacobian\")\n",
    "\n",
    "    \"\"\"\n",
    "    log_like_se, log_like_var = choose_case(\n",
    "        log_like_obs, params, log_like_kwargs, design_options, cov_type\n",
    "    )\n",
    "    model_inference_table, params_cov = inference_table(\n",
    "        params, log_like_se, log_like_var, cov_type\n",
    "    )\n",
    "    return model_inference_table, params_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1\n",
    "### Logit illustration with design options not specified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When design options are not specified, your model has access to three variance estimators: (1) Robust or \"Sandwich\" estimator (2) Observed Information Matrix (3) Outer Product of Gradients. These are explained in the background section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(              value  sandwich_standard_error  ci_lower  ci_upper\n",
       " Intercept  0.965938                 0.047480  0.872879  1.058998\n",
       " ppltrst    0.010980                 0.006770 -0.002289  0.024248\n",
       " male      -0.189040                 0.031561 -0.250899 -0.127181\n",
       " income    -0.006447                 0.005907 -0.018023  0.005130,\n",
       "               value  sandwich_standard_errors  ci_lower  ci_upper\n",
       " Intercept  0.965938                  0.047480  0.872877  1.058999\n",
       " ppltrst    0.010980                  0.006770 -0.002289  0.024248\n",
       " male      -0.189040                  0.031561 -0.250901 -0.127180\n",
       " income    -0.006447                  0.005906 -0.018024  0.005130)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define design_options and parameters dataframe\n",
    "design_options = pd.DataFrame()\n",
    "params = pd.DataFrame(\n",
    "    data=[0.9659383, 0.0109796, -0.1890401, -0.0064468],\n",
    "    index=[\"Intercept\", \"ppltrst\", \"male\", \"income\"],\n",
    "    columns=[\"value\"],\n",
    ")\n",
    "\n",
    "# Running a logit model with design options not specified, robust\n",
    "inf_table, params_cov = likelihood_inference(\n",
    "    logit, params, logit_kwargs, design_options, cov_type=\"sandwich\"\n",
    ")\n",
    "\n",
    "# Stata Results\n",
    "stata_params_dict = {\n",
    "    \"value\": [0.9659383, 0.0109796, -0.1890401, -0.0064468],\n",
    "    \"sandwich_standard_error\": [0.0474801, 0.0067696, 0.0315615, 0.0059065],\n",
    "    \"ci_lower\": [0.8728789, -0.0022886, -0.2508995, -0.0180233],\n",
    "    \"ci_upper\": [1.058998, 0.0242478, -0.1271807, 0.0051297],\n",
    "}\n",
    "stata_params_df = pd.DataFrame(\n",
    "    stata_params_dict, index=[\"Intercept\", \"ppltrst\", \"male\", \"income\"]\n",
    ")\n",
    "stata_params_df, inf_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2\n",
    "### Logit illustration with primary sampling units or \"clusters\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose your data has primary sampling units (psu) or \"clusters\". You may specify the cluster variable in the `inference_design_options`. Again, we take the estimated parameters as given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(              value  sandwich_standard_error  ci_lower  ci_upper\n",
       " Intercept  0.965938                 0.050478  0.866993  1.064883\n",
       " ppltrst    0.010980                 0.007137 -0.003010  0.024969\n",
       " male      -0.189040                 0.031800 -0.251374 -0.126706\n",
       " income    -0.006447                 0.006466 -0.019122  0.006228,\n",
       "               value  sandwich_standard_errors  ci_lower  ci_upper\n",
       " Intercept  0.965938                  0.050478  0.867002  1.064874\n",
       " ppltrst    0.010980                  0.007137 -0.003009  0.024968\n",
       " male      -0.189040                  0.031800 -0.251368 -0.126712\n",
       " income    -0.006447                  0.006466 -0.019121  0.006227)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define design_options and parameters dataframe\n",
    "inference_design_options = pd.DataFrame({\"psu\": data[\"psu\"]})\n",
    "params = pd.DataFrame(\n",
    "    data=[0.9659383, 0.0109796, -0.1890401, -0.0064468],\n",
    "    index=[\"Intercept\", \"ppltrst\", \"male\", \"income\"],\n",
    "    columns=[\"value\"],\n",
    ")\n",
    "\n",
    "# Running a logit model with design options not specified, robust\n",
    "inf_table, params_cov = likelihood_inference(\n",
    "    logit, params, logit_kwargs, inference_design_options, cov_type=\"sandwich\"\n",
    ")\n",
    "\n",
    "# Stata Results\n",
    "stata_params_dict = {\n",
    "    \"value\": [0.9659383, 0.0109796, -0.1890401, -0.0064468],\n",
    "    \"sandwich_standard_error\": [0.0504775, 0.0071368, 0.0318001, 0.0064663],\n",
    "    \"ci_lower\": [0.8669933, -0.0030098, -0.2513741, -0.0191218],\n",
    "    \"ci_upper\": [1.064883, 0.024969, -0.1267061, 0.0062283],\n",
    "}\n",
    "stata_params_df = pd.DataFrame(\n",
    "    stata_params_dict, index=[\"Intercept\", \"ppltrst\", \"male\", \"income\"]\n",
    ")\n",
    "stata_params_df, inf_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to running the model without any design specifications, the standard errors have jumped. This is expected, given observations are no longer independent; only independent between *clusters*. The magnitude of the jump is small here simply because there are 11,015 clusters and 19,751 observations. As the number of clusters approach the size of the data, it would approach the standard errors for independent observations. Likewise, we can expect a larger jump if less clusters are defined. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3\n",
    "### Probit illustration with psu and strata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following illustration, we specify the psu and the strata. In case stratum have just one cluster, we use the \"grand mean\" method. More on this in the background. Finally, when clusters or strata are defined, only the robust or \"sandwich\" estimation is possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "\n",
    "def probit(params, y, x, design_options):\n",
    "    \"\"\"Refer to logit docstring for details!\"\"\"\n",
    "    q = 2 * y - 1\n",
    "    c = np.log(stats.norm._cdf(np.dot(q[:, None] * x, params[\"value\"])))\n",
    "    if \"weight\" in design_options.columns:\n",
    "        return c * design_options[\"weight\"].to_numpy()\n",
    "    else:\n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jrxz12\\Anaconda3\\envs\\estimagic\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: divide by zero encountered in log\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(              value  sandwich_standard_error  ci_lower  ci_upper\n",
       " Intercept  0.595919                 0.029567  0.537962  0.653876\n",
       " ppltrst    0.006508                 0.004221 -0.001765  0.014782\n",
       " male      -0.113632                 0.018908 -0.150695 -0.076569\n",
       " income    -0.003856                 0.003812 -0.011329  0.003617,\n",
       "               value  sandwich_standard_errors  ci_lower  ci_upper\n",
       " Intercept  0.595919                  0.029589  0.537924  0.653914\n",
       " ppltrst    0.006508                  0.004228 -0.001779  0.014795\n",
       " male      -0.113632                  0.018975 -0.150824 -0.076440\n",
       " income    -0.003856                  0.003819 -0.011342  0.003630)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define design_options and parameters dataframe\n",
    "inference_design_options = pd.DataFrame({\"psu\": data[\"psu\"], \"strata\": data[\"stratum\"]})\n",
    "probit_design_options = pd.DataFrame()\n",
    "params = pd.DataFrame(\n",
    "    data=[0.595919, 0.0065084, -0.1136318, -0.0038559],\n",
    "    index=[\"Intercept\", \"ppltrst\", \"male\", \"income\"],\n",
    "    columns=[\"value\"],\n",
    ")\n",
    "\n",
    "# Defining probit keyword arguments.\n",
    "probit_kwargs = {\"y\": y, \"x\": x, \"design_options\": probit_design_options}\n",
    "inf_table, params_cov = likelihood_inference(\n",
    "    probit, params, probit_kwargs, inference_design_options, cov_type=\"sandwich\"\n",
    ")\n",
    "\n",
    "# For probit with psu, strata, robust\n",
    "stata_params_dict = {\n",
    "    \"value\": [0.595919, 0.0065084, -0.1136318, -0.0038559],\n",
    "    \"sandwich_standard_error\": [0.029567, 0.0042209, 0.0189078, 0.0038124],\n",
    "    \"ci_lower\": [0.5379617, -0.0017655, -0.1506948, -0.0113289],\n",
    "    \"ci_upper\": [0.6538763, 0.0147822, -0.0765688, 0.0036172],\n",
    "}\n",
    "stata_params_df = pd.DataFrame(\n",
    "    stata_params_dict, index=[\"Intercept\", \"ppltrst\", \"male\", \"income\"]\n",
    ")\n",
    "stata_params_df, inf_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
